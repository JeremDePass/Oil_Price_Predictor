{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNhfmLb+kpxGR+Fjy8u/a6r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["These is a web scraped designed to work in distributed fashion and is used the process of scaping data is made faster by using the multithreading.\n","\n","It could be made even faster using rotating proxies which can change IP per request and helps you getting detected as a bot by the website owner. But, to respect them and not accidently DDOS attacking them scrape with caution."],"metadata":{"id":"MWIWe7gk-KZf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8bQEapyx8Xb5"},"outputs":[],"source":["import asyncio\n","import csv\n","import os\n","from datetime import datetime\n","from concurrent.futures import ThreadPoolExecutor\n","from playwright.async_api import async_playwright"]},{"cell_type":"code","source":["# CONFIG\n","OUTPUT_FILE = \"boereport_articles.csv\"\n","BASE_URL = \"https://boereport.com/category/oil-and-gas-news-headlines/page/{}\"\n","START_PAGE = 1\n","END_PAGE = 3388\n","CONCURRENCY = max(2, os.cpu_count())"],"metadata":{"id":"uOhTsQWI8q1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scrape one page\n","async def scrape_single_page(browser, page_number):\n","    url = BASE_URL.format(page_number)\n","    try:\n","        page = await browser.new_page()\n","        await page.goto(url, timeout=120000)\n","        await page.wait_for_selector(\"article\", timeout=20000)\n","\n","        articles = await page.query_selector_all(\"article\")\n","        results = []\n","\n","        for article in articles:\n","            title_el = await article.query_selector(\"h2.entry-title a\")\n","            date_el = await article.query_selector(\"time.entry-time\")\n","            desc_el = await article.query_selector(\".entry-content p\")\n","\n","            title = (await title_el.inner_text()) if title_el else \"\"\n","            date_raw = (await date_el.get_attribute(\"datetime\")) if date_el else \"\"\n","            description = (await desc_el.inner_text()) if desc_el else \"\"\n","\n","            # Extract only the date (YYYY-MM-DD)\n","            date = \"\"\n","            if date_raw:\n","                try:\n","                    date = datetime.fromisoformat(date_raw.replace(\"Z\", \"\")).date().isoformat()\n","                except ValueError:\n","                    if \"T\" in date_raw:\n","                        date = date_raw.split(\"T\")[0]\n","\n","            if title and date:\n","                results.append({\n","                    \"date\": date.strip(),\n","                    \"title\": title.strip(),\n","                    \"description\": description.strip()\n","                })\n","\n","        await page.close()\n","        print(f\"Page {page_number}: {len(results)} articles\")\n","        return results\n","\n","    except Exception as e:\n","        print(f\"Error scraping page {page_number}: {e}\")\n","        return []"],"metadata":{"id":"lAEaGKL88vTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# WORKER GROUP FUNCTION\n","async def scrape_pages_group(playwright, page_numbers):\n","    browser = await playwright.firefox.launch(headless=True)\n","    group_results = []\n","\n","    for num in page_numbers:\n","        res = await scrape_single_page(browser, num)\n","        group_results.extend(res)\n","\n","    await browser.close()\n","    return group_results"],"metadata":{"id":"vjdPEBgS9J2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PARALLEL COORDINATOR\n","async def main():\n","    os.makedirs(os.path.dirname(OUTPUT_FILE) or \".\", exist_ok=True)\n","    file_exists = os.path.exists(OUTPUT_FILE)\n","\n","    async with async_playwright() as p:\n","        # divide pages into batches based on concurrency\n","        all_pages = list(range(START_PAGE, END_PAGE + 1))\n","        page_groups = [all_pages[i:i + CONCURRENCY] for i in range(0, len(all_pages), CONCURRENCY)]\n","\n","        # open CSV file once\n","        with open(OUTPUT_FILE, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n","            writer = csv.DictWriter(csvfile, fieldnames=[\"date\", \"title\", \"description\"])\n","            if not file_exists:\n","                writer.writeheader()\n","\n","            # process groups sequentially, each group runs pages concurrently\n","            for idx, group in enumerate(page_groups, start=1):\n","                print(f\"\\nProcessing batch {idx}/{len(page_groups)} with {len(group)} pages...\")\n","\n","                # run this group in parallel using asyncio.gather\n","                results_per_group = await asyncio.gather(*[scrape_pages_group(p, [num]) for num in group])\n","\n","                # flatten all results\n","                all_results = [r for group_res in results_per_group for r in group_res]\n","\n","                if all_results:\n","                    writer.writerows(all_results)\n","                    print(f\"Saved {len(all_results)} articles from batch {idx}\")\n","                else:\n","                    print(f\"No data from batch {idx}\")\n","\n","    print(f\"\\nDONE â€” Data saved to {OUTPUT_FILE}\")"],"metadata":{"id":"Lj7FJZf99Ly5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    asyncio.run(main())"],"metadata":{"id":"ogx8p4dd9OcN"},"execution_count":null,"outputs":[]}]}